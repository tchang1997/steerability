probe: "./data/steerbench_v2.1.csv"
prompt_strategy: "direct"
llm_settings:
    llm_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
    chat_type: "vllm"
    cache_file: "vllm-deepseek-8b-v2.1.tsv"
    other_kwargs:
        port: 16384 
        max_simul_calls: 128 # 256 concurrent is the max we seem to be able to handle (max_simul_calls * num_generations)
        timeout: 6000
        text_gen_kwargs: # in sammo-language
            randomness: 0.0
            num_generations: 1
        max_tokens: 4096 # same as main probe
inst_addons:
  disambig: True
experiment_name: "v2/steerbench_v2.1_deepseek_8b_negprompt"
seed: 42
