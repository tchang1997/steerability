probe: "./data/steerbench_converted.csv"
prompt_strategy: "direct"
llm_settings:
    llm_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
    chat_type: "vllm"
    cache_file: "vllm-deepseek.tsv"
    other_kwargs:
        port: 5000
        max_simul_calls: 24
        timeout: 3600 # turn this down -- one point in particular is troublesome
experiment_name: "deepseek_distill_70b_local"
seed: 42
